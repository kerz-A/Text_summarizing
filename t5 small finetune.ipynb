{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T22:02:33.398620Z",
     "iopub.status.busy": "2024-12-24T22:02:33.398236Z",
     "iopub.status.idle": "2024-12-24T22:02:33.402762Z",
     "shell.execute_reply": "2024-12-24T22:02:33.401882Z",
     "shell.execute_reply.started": "2024-12-24T22:02:33.398581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T22:02:33.407772Z",
     "iopub.status.busy": "2024-12-24T22:02:33.407509Z",
     "iopub.status.idle": "2024-12-24T22:02:34.854764Z",
     "shell.execute_reply": "2024-12-24T22:02:34.853794Z",
     "shell.execute_reply.started": "2024-12-24T22:02:33.407750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n",
      "Загрузка датасета...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'highlights', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "print(\"Using GPU:\", torch.cuda.is_available())\n",
    "\n",
    "print(\"Загрузка датасета...\")\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T22:02:34.856076Z",
     "iopub.status.busy": "2024-12-24T22:02:34.855770Z",
     "iopub.status.idle": "2024-12-24T22:10:12.021881Z",
     "shell.execute_reply": "2024-12-24T22:10:12.021207Z",
     "shell.execute_reply.started": "2024-12-24T22:02:34.856051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизация данных...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee7966a138e4db782dde6bf2cf11dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe70af8e2ce447e9461b9132695a591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ba61cbff944a619c361f919bea9de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868f99f3563c47acb94ceedcdf62bb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1837a3b2ef3e41668ec9c9152afb516d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cacb92715e4e4e98471efcd35763f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Токенизация данных...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(examples[\"highlights\"], max_length=32, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "train_data = tokenized_dataset[\"train\"].select(range(len(tokenized_dataset[\"train\"]) // 2))\n",
    "val_data = tokenized_dataset[\"validation\"].select(range(len(tokenized_dataset[\"validation\"]) // 2))\n",
    "test_data = tokenized_dataset[\"test\"].select(range(len(tokenized_dataset[\"test\"]) // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T22:10:12.023973Z",
     "iopub.status.busy": "2024-12-24T22:10:12.023750Z",
     "iopub.status.idle": "2024-12-24T22:10:13.731488Z",
     "shell.execute_reply": "2024-12-24T22:10:13.730407Z",
     "shell.execute_reply.started": "2024-12-24T22:10:12.023954Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7377d95de3094e959ab2d96f99129adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde450882353485cb81dda50e1a826c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a9a52b97bc406b981bd8425d829d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Загрузка модели...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T22:10:13.733420Z",
     "iopub.status.busy": "2024-12-24T22:10:13.733100Z",
     "iopub.status.idle": "2024-12-24T22:10:13.740570Z",
     "shell.execute_reply": "2024-12-24T22:10:13.739823Z",
     "shell.execute_reply.started": "2024-12-24T22:10:13.733388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        scores = scorer.score(label, pred)\n",
    "        rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "        rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "        rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": sum(rouge1_scores) / len(rouge1_scores),\n",
    "        \"rouge2\": sum(rouge2_scores) / len(rouge2_scores),\n",
    "        \"rougeL\": sum(rougeL_scores) / len(rougeL_scores),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T22:10:13.741783Z",
     "iopub.status.busy": "2024-12-24T22:10:13.741483Z",
     "iopub.status.idle": "2024-12-24T22:10:13.785838Z",
     "shell.execute_reply": "2024-12-24T22:10:13.785016Z",
     "shell.execute_reply.started": "2024-12-24T22:10:13.741753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Настройка параметров обучения...\n"
     ]
    }
   ],
   "source": [
    "print(\"Настройка параметров обучения...\")\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=7,\n",
    "    gradient_accumulation_steps=8,\n",
    "    predict_with_generate=True,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    log_level=\"info\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T22:10:13.787028Z",
     "iopub.status.busy": "2024-12-24T22:10:13.786738Z",
     "iopub.status.idle": "2024-12-24T22:10:13.896027Z",
     "shell.execute_reply": "2024-12-24T22:10:13.895152Z",
     "shell.execute_reply.started": "2024-12-24T22:10:13.786998Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T22:10:13.897339Z",
     "iopub.status.busy": "2024-12-24T22:10:13.897045Z",
     "iopub.status.idle": "2024-12-25T02:57:54.954111Z",
     "shell.execute_reply": "2024-12-25T02:57:54.953201Z",
     "shell.execute_reply.started": "2024-12-24T22:10:13.897315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начало обучения...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 143,556\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 31,402\n",
      "  Number of trainable parameters = 60,506,624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31402' max='31402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31402/31402 4:47:40, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.749200</td>\n",
       "      <td>2.163796</td>\n",
       "      <td>0.315993</td>\n",
       "      <td>0.149637</td>\n",
       "      <td>0.269477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.705000</td>\n",
       "      <td>2.135216</td>\n",
       "      <td>0.316703</td>\n",
       "      <td>0.149180</td>\n",
       "      <td>0.270056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.650600</td>\n",
       "      <td>2.126991</td>\n",
       "      <td>0.317847</td>\n",
       "      <td>0.150921</td>\n",
       "      <td>0.271172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.622400</td>\n",
       "      <td>2.113619</td>\n",
       "      <td>0.317381</td>\n",
       "      <td>0.151159</td>\n",
       "      <td>0.270975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.625600</td>\n",
       "      <td>2.103940</td>\n",
       "      <td>0.318482</td>\n",
       "      <td>0.152007</td>\n",
       "      <td>0.271794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.625700</td>\n",
       "      <td>2.100536</td>\n",
       "      <td>0.318585</td>\n",
       "      <td>0.151775</td>\n",
       "      <td>0.271597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.608600</td>\n",
       "      <td>2.096549</td>\n",
       "      <td>0.318511</td>\n",
       "      <td>0.151635</td>\n",
       "      <td>0.271466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Configuration saved in ./results/checkpoint-500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-500/spiece.model\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Configuration saved in ./results/checkpoint-1000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-1000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-1000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-375] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Configuration saved in ./results/checkpoint-1500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-1500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-1500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Configuration saved in ./results/checkpoint-2000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-2000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-2000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Configuration saved in ./results/checkpoint-2500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-2500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-2500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n",
      "Configuration saved in ./results/checkpoint-3000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-3000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-3000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n",
      "Configuration saved in ./results/checkpoint-3500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-3500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-3500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n",
      "Configuration saved in ./results/checkpoint-4000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-4000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-4000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6684\n",
      "  Batch size = 4\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./results/checkpoint-4500\n",
      "Configuration saved in ./results/checkpoint-4500/config.json\n",
      "Configuration saved in ./results/checkpoint-4500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-4500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-4500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n",
      "Configuration saved in ./results/checkpoint-5000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-5000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-5000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-5500\n",
      "Configuration saved in ./results/checkpoint-5500/config.json\n",
      "Configuration saved in ./results/checkpoint-5500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-5500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-5500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-6000\n",
      "Configuration saved in ./results/checkpoint-6000/config.json\n",
      "Configuration saved in ./results/checkpoint-6000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-6000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-6000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-6500\n",
      "Configuration saved in ./results/checkpoint-6500/config.json\n",
      "Configuration saved in ./results/checkpoint-6500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-6500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-6500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-7000\n",
      "Configuration saved in ./results/checkpoint-7000/config.json\n",
      "Configuration saved in ./results/checkpoint-7000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-7000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-7000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-7500\n",
      "Configuration saved in ./results/checkpoint-7500/config.json\n",
      "Configuration saved in ./results/checkpoint-7500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-7500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-7500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-8000\n",
      "Configuration saved in ./results/checkpoint-8000/config.json\n",
      "Configuration saved in ./results/checkpoint-8000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-8000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-8000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-8500\n",
      "Configuration saved in ./results/checkpoint-8500/config.json\n",
      "Configuration saved in ./results/checkpoint-8500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-8500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-8500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6684\n",
      "  Batch size = 4\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./results/checkpoint-9000\n",
      "Configuration saved in ./results/checkpoint-9000/config.json\n",
      "Configuration saved in ./results/checkpoint-9000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-9000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-9000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-9500\n",
      "Configuration saved in ./results/checkpoint-9500/config.json\n",
      "Configuration saved in ./results/checkpoint-9500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-9500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-9500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-10000\n",
      "Configuration saved in ./results/checkpoint-10000/config.json\n",
      "Configuration saved in ./results/checkpoint-10000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-10000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-10000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-10000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-10500\n",
      "Configuration saved in ./results/checkpoint-10500/config.json\n",
      "Configuration saved in ./results/checkpoint-10500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-10500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-10500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-10500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-11000\n",
      "Configuration saved in ./results/checkpoint-11000/config.json\n",
      "Configuration saved in ./results/checkpoint-11000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-11000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-11000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-11000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-11500\n",
      "Configuration saved in ./results/checkpoint-11500/config.json\n",
      "Configuration saved in ./results/checkpoint-11500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-11500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-11500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-11500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-12000\n",
      "Configuration saved in ./results/checkpoint-12000/config.json\n",
      "Configuration saved in ./results/checkpoint-12000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-12000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-12000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-12000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-12500\n",
      "Configuration saved in ./results/checkpoint-12500/config.json\n",
      "Configuration saved in ./results/checkpoint-12500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-12500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-12500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-12500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-13000\n",
      "Configuration saved in ./results/checkpoint-13000/config.json\n",
      "Configuration saved in ./results/checkpoint-13000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-13000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-13000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-13000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-12000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6684\n",
      "  Batch size = 4\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./results/checkpoint-13500\n",
      "Configuration saved in ./results/checkpoint-13500/config.json\n",
      "Configuration saved in ./results/checkpoint-13500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-13500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-13500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-13500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-14000\n",
      "Configuration saved in ./results/checkpoint-14000/config.json\n",
      "Configuration saved in ./results/checkpoint-14000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-14000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-14000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-14000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-14500\n",
      "Configuration saved in ./results/checkpoint-14500/config.json\n",
      "Configuration saved in ./results/checkpoint-14500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-14500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-14500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-14500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-15000\n",
      "Configuration saved in ./results/checkpoint-15000/config.json\n",
      "Configuration saved in ./results/checkpoint-15000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-15000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-15000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-15000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-15500\n",
      "Configuration saved in ./results/checkpoint-15500/config.json\n",
      "Configuration saved in ./results/checkpoint-15500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-15500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-15500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-15500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-16000\n",
      "Configuration saved in ./results/checkpoint-16000/config.json\n",
      "Configuration saved in ./results/checkpoint-16000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-16000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-16000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-16000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-16500\n",
      "Configuration saved in ./results/checkpoint-16500/config.json\n",
      "Configuration saved in ./results/checkpoint-16500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-16500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-16500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-16500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-17000\n",
      "Configuration saved in ./results/checkpoint-17000/config.json\n",
      "Configuration saved in ./results/checkpoint-17000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-17000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-17000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-17000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-17500\n",
      "Configuration saved in ./results/checkpoint-17500/config.json\n",
      "Configuration saved in ./results/checkpoint-17500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-17500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-17500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-17500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-16500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6684\n",
      "  Batch size = 4\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./results/checkpoint-18000\n",
      "Configuration saved in ./results/checkpoint-18000/config.json\n",
      "Configuration saved in ./results/checkpoint-18000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-18000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-18000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-18000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-18500\n",
      "Configuration saved in ./results/checkpoint-18500/config.json\n",
      "Configuration saved in ./results/checkpoint-18500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-18500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-18500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-18500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-19000\n",
      "Configuration saved in ./results/checkpoint-19000/config.json\n",
      "Configuration saved in ./results/checkpoint-19000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-19000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-19000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-19000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-19500\n",
      "Configuration saved in ./results/checkpoint-19500/config.json\n",
      "Configuration saved in ./results/checkpoint-19500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-19500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-19500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-19500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-20000\n",
      "Configuration saved in ./results/checkpoint-20000/config.json\n",
      "Configuration saved in ./results/checkpoint-20000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-20000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-20000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-20000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-20500\n",
      "Configuration saved in ./results/checkpoint-20500/config.json\n",
      "Configuration saved in ./results/checkpoint-20500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-20500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-20500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-20500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-21000\n",
      "Configuration saved in ./results/checkpoint-21000/config.json\n",
      "Configuration saved in ./results/checkpoint-21000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-21000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-21000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-21000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-21500\n",
      "Configuration saved in ./results/checkpoint-21500/config.json\n",
      "Configuration saved in ./results/checkpoint-21500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-21500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-21500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-21500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-22000\n",
      "Configuration saved in ./results/checkpoint-22000/config.json\n",
      "Configuration saved in ./results/checkpoint-22000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-22000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-22000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-22000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-21000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6684\n",
      "  Batch size = 4\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./results/checkpoint-22500\n",
      "Configuration saved in ./results/checkpoint-22500/config.json\n",
      "Configuration saved in ./results/checkpoint-22500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-22500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-22500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-22500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-23000\n",
      "Configuration saved in ./results/checkpoint-23000/config.json\n",
      "Configuration saved in ./results/checkpoint-23000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-23000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-23000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-23000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-23500\n",
      "Configuration saved in ./results/checkpoint-23500/config.json\n",
      "Configuration saved in ./results/checkpoint-23500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-23500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-23500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-23500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-24000\n",
      "Configuration saved in ./results/checkpoint-24000/config.json\n",
      "Configuration saved in ./results/checkpoint-24000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-24000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-24000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-24000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-24500\n",
      "Configuration saved in ./results/checkpoint-24500/config.json\n",
      "Configuration saved in ./results/checkpoint-24500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-24500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-24500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-24500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-25000\n",
      "Configuration saved in ./results/checkpoint-25000/config.json\n",
      "Configuration saved in ./results/checkpoint-25000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-25000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-25000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-25000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-25500\n",
      "Configuration saved in ./results/checkpoint-25500/config.json\n",
      "Configuration saved in ./results/checkpoint-25500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-25500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-25500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-25500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-26000\n",
      "Configuration saved in ./results/checkpoint-26000/config.json\n",
      "Configuration saved in ./results/checkpoint-26000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-26000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-26000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-26000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-26500\n",
      "Configuration saved in ./results/checkpoint-26500/config.json\n",
      "Configuration saved in ./results/checkpoint-26500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-26500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-26500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-26500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-25500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6684\n",
      "  Batch size = 4\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./results/checkpoint-27000\n",
      "Configuration saved in ./results/checkpoint-27000/config.json\n",
      "Configuration saved in ./results/checkpoint-27000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-27000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-27000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-27000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-27500\n",
      "Configuration saved in ./results/checkpoint-27500/config.json\n",
      "Configuration saved in ./results/checkpoint-27500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-27500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-27500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-27500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-28000\n",
      "Configuration saved in ./results/checkpoint-28000/config.json\n",
      "Configuration saved in ./results/checkpoint-28000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-28000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-28000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-28000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-28500\n",
      "Configuration saved in ./results/checkpoint-28500/config.json\n",
      "Configuration saved in ./results/checkpoint-28500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-28500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-28500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-28500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-29000\n",
      "Configuration saved in ./results/checkpoint-29000/config.json\n",
      "Configuration saved in ./results/checkpoint-29000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-29000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-29000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-29000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-29500\n",
      "Configuration saved in ./results/checkpoint-29500/config.json\n",
      "Configuration saved in ./results/checkpoint-29500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-29500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-29500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-29500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-30000\n",
      "Configuration saved in ./results/checkpoint-30000/config.json\n",
      "Configuration saved in ./results/checkpoint-30000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-30000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-30000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-30000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-30500\n",
      "Configuration saved in ./results/checkpoint-30500/config.json\n",
      "Configuration saved in ./results/checkpoint-30500/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-30500/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-30500/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-30500/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-31000\n",
      "Configuration saved in ./results/checkpoint-31000/config.json\n",
      "Configuration saved in ./results/checkpoint-31000/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-31000/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-31000/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-31000/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-31402\n",
      "Configuration saved in ./results/checkpoint-31402/config.json\n",
      "Configuration saved in ./results/checkpoint-31402/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-31402/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-31402/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-31402/special_tokens_map.json\n",
      "Copy vocab file to ./results/checkpoint-31402/spiece.model\n",
      "Deleting older checkpoint [results/checkpoint-30500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6684\n",
      "  Batch size = 4\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31402, training_loss=2.67335812453745, metrics={'train_runtime': 17260.6821, 'train_samples_per_second': 58.219, 'train_steps_per_second': 1.819, 'total_flos': 3.400002599858995e+16, 'train_loss': 2.67335812453745, 'epoch': 6.999804954164229})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Начало обучения...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T02:57:54.956102Z",
     "iopub.status.busy": "2024-12-25T02:57:54.955861Z",
     "iopub.status.idle": "2024-12-25T03:04:05.081631Z",
     "shell.execute_reply": "2024-12-25T03:04:05.080745Z",
     "shell.execute_reply.started": "2024-12-25T02:57:54.956081Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: article, id, highlights. If article, id, highlights are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5745\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка на тестовой выборке...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1437' max='1437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1437/1437 06:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Оценка на тестовой выборке...\")\n",
    "metrics = trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T03:04:05.083027Z",
     "iopub.status.busy": "2024-12-25T03:04:05.082724Z",
     "iopub.status.idle": "2024-12-25T03:04:05.089706Z",
     "shell.execute_reply": "2024-12-25T03:04:05.088838Z",
     "shell.execute_reply.started": "2024-12-25T03:04:05.082998Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "ROUGE-1: 0.3173\n",
      "ROUGE-2: 0.1514\n",
      "ROUGE-L: 0.2710\n",
      "Loss: 2.1118\n",
      "Runtime (s): 370.1155\n",
      "Samples per Second: 15.52\n",
      "Steps per Second: 3.88\n"
     ]
    }
   ],
   "source": [
    "def print_metrics(metrics):\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(f\"ROUGE-1: {metrics['eval_rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {metrics['eval_rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {metrics['eval_rougeL']:.4f}\")\n",
    "    print(f\"Loss: {metrics['eval_loss']:.4f}\")\n",
    "    print(f\"Runtime (s): {metrics['eval_runtime']:.4f}\")\n",
    "    print(f\"Samples per Second: {metrics['eval_samples_per_second']:.2f}\")\n",
    "    print(f\"Steps per Second: {metrics['eval_steps_per_second']:.2f}\")\n",
    "\n",
    "print_metrics(metrics)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

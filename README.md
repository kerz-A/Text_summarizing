## Описание проекта

Проект посвящён автоматической суммаризации текста с использованием предобученной модели T5-Small. Основная цель — оптимизировать модели суммаризации, которые преобразуют длинные статьи в краткие и информативные обзоры, сохраняя ключевые идеи и важные моменты исходного материала.

## Цели проекта

- Разработать модель суммаризации текста на основе трансформеров.
- Обеспечить вычислительную эффективность для работы с ограниченными ресурсами.
- Оценить качество результатов с использованием метрик ROUGE.

## Используемые данные

- **Датасет:** CNN/DailyMail
  - Статьи и их референсные суммаризации.
  - Структура данных:
    - `article`: текст статьи.
    - `highlights`: референсные ключевые моменты.
    - `id`: уникальный идентификатор статьи.
  - Размер данных: обучающая, валидационная и тестовая выборки были уменьшены до 50% для ускорения обучения.

## Выбранная модель

- **T5-Small:**
  - Компактная модель, оптимизированная для ограниченных вычислительных ресурсов.
  - Преимущества:
    - Меньше параметров, что снижает требования к памяти.
    - Быстрая итерация и экспериментирование.
  - Поддерживает задачи преобразования текста, такие как суммаризация.

## Методы и подходы

1. **Предобработка данных:**
   - Токенизация текста.
   - Ограничение длины токенов (128 для входного текста и 32 для выхода).

2. **Обучение:**
   - Использование библиотеки Hugging Face и `Seq2SeqTrainer`.
   - Градиентное накопление для эффективного использования ресурсов.
   - Использование смешанной точности (FP16) для ускорения обучения.
   - Тюнинг гиперпараметров:
     - Уменьшение размера батча.
     - Увеличение количества эпох (7 вместо стандартных 3-5).

3. **Метрики оценки:**
   - ROUGE-1: совпадение на уровне слов.
   - ROUGE-2: совпадение на уровне биграмм.
   - ROUGE-L: совпадение длиннейшей общей подстроки.

## Результаты

- **Дообученная модель T5-Small:**
  - ROUGE-1: 31.73%
  - ROUGE-2: 15.14%
  - ROUGE-L: 27.10%
- **Время обучения:**
  - Полное время: ~4 часа 47 минут.
  - Обработка: 15.52 примеров/сек, 3.88 шагов/сек.

## Основные выводы

- **Что получилось:**
  - Успешная настройка модели на уменьшенном датасете.
  - Стабильные метрики ROUGE, подходящие для практического использования.
  - Снижение потерь на валидации до 2.0965.

- **Что не получилось:**
  - Невозможность использования T5-Base из-за ограничений GPU.
  - Средние метрики ROUGE, обусловленные ограничениями ресурсов.

## Возможные улучшения

- Увеличение тренировочного набора данных.
- Эксперименты с другими архитектурами, такими как BART.
- Оптимизация гиперпараметров для повышения качества суммаризации.

## Используемые библиотеки

- `pandas`
- `numpy`
- `transformers` (Hugging Face)
- `torch`
- `seaborn`
- `matplotlib`

## Навыки, продемонстрированные в проекте

- Работа с предобученными моделями NLP.
- Оптимизация вычислений (градиентное накопление, смешанная точность).
- Использование метрик ROUGE для оценки качества суммаризации.
- Анализ результатов и поиск путей улучшения.
